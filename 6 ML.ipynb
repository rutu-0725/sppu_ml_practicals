{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e77666-42fd-427e-83ab-7067f47ea809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the maze environment\n",
    "maze = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, -1, -1, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, -1, -1, -1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "])\n",
    "\n",
    "start_state = (0, 0)\n",
    "goal_state = (4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f876db2-894d-40b7-9fdb-6a0f70056b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.9\n",
    "max_episodes = 1000\n",
    "max_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2293e9de-3842-45cc-8811-e517a5c13e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-table\n",
    "num_rows, num_cols = maze.shape\n",
    "# 4 possible actions: up, down, left, right\n",
    "num_actions = 4\n",
    "Q = np.zeros((num_rows, num_cols, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa3a6e2-6434-419c-a19f-e7825e34e094",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -6 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# penalize for hitting walls or going out of bounds\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Update the Q-value for the previous state-action pair\u001b[39;00m\n\u001b[0;32m     29\u001b[0m Q[state][action] \u001b[38;5;241m=\u001b[39m Q[state][action] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m---> 30\u001b[0m     reward \u001b[38;5;241m+\u001b[39m discount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q[next_state]) \u001b[38;5;241m-\u001b[39m Q[state][action]\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Update the current state\u001b[39;00m\n\u001b[0;32m     34\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mIndexError\u001b[0m: index -6 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(max_episodes):\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        # Choose an action using epsilon-greedy policy\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.randint(num_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        if action == 0:  # up\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:  # down\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 2:  # left\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        elif action == 3:  # right\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "\n",
    "        # Check if the next state is valid\n",
    "        if 0 <= next_state[0] < num_rows and 0 <= next_state[1] < num_cols:\n",
    "            reward = maze[next_state]\n",
    "        else:\n",
    "            reward = -5  # penalize for hitting walls or going out of bounds\n",
    "\n",
    "        # Update the Q-value for the previous state-action pair\n",
    "        Q[state][action] = Q[state][action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(Q[next_state]) - Q[state][action]\n",
    "        )\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the agent reached the goal state\n",
    "        if state == goal_state:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Steps: {step + 1}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab48bd9-e559-484b-a768-1c4d3f1267e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results - Steps: 0, Success Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing the learned policy\n",
    "state = start_state\n",
    "steps = 0\n",
    "success_count = 0\n",
    "while state != goal_state and steps < max_steps:\n",
    "    action = np.argmax(Q[state])\n",
    "    if action == 0:  # up\n",
    "        next_state = (state[0] - 1, state[1])\n",
    "    elif action == 1:  # down\n",
    "        next_state = (state[0] + 1, state[1])\n",
    "    elif action == 2:  # left\n",
    "        next_state = (state[0], state[1] - 1)\n",
    "    elif action == 3:  # right\n",
    "        next_state = (state[0], state[1] + 1)\n",
    "\n",
    "    if 0 <= next_state[0] < num_rows and 0 <= next_state[1] < num_cols:\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if state == goal_state:\n",
    "            success_count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Testing Results - Steps: {steps}, Success Rate: {success_count / max_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9faf833-8f57-4840-ac1f-f700a750c23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
